{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30027 Machine Learning Assignment 2\n",
    "\n",
    "## Description of text features\n",
    "\n",
    "This notebook describes the pre-computed text features provided for assignment 2. **You do not need to recompute the features yourself for this assignment** -- this information is just for your reference. However, feel free to experiment with different text features if you are interested. If you do want to try generating your own text features, some things to keep in mind:\n",
    "- There are many different decisions you can make throughout the feature design process, from the text preprocessing to the size of the output vectors. There's no guarantee that the defaults we chose will produce the best possible text features for this classification task, so feel free to experiment with different settings.\n",
    "- These features must be trained on a training corpus. Generally, the training corpus should not include validation samples, but for the purposes of this assignment we have used the entire non-test set (training+validation) as the training corpus, to allow you to experiment with different validation sets. If you recompute the text features as part of your own model, you should exclude validation samples and retrain on training samples only. Note that if you do N-fold cross-validation, this means generating N sets of features for N different training-validation splits.\n",
    "- This code may take a long time to run and require a good bit of memory, which is why we are not requiring you to recompute these features yourself. doc2vec in particular is very slow unless you can implement some speed-ups in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read recipe_train.csv\n",
    "x_train_original = pd.read_csv(r\"recipe_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "# use recipe name as an example\n",
    "train_corpus_name = x_train_original['name']\n",
    "test_name = x_train_original['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer\n",
    "\n",
    "A count vectorizer converts documents to vectors which represent word counts. Each column in the output represents a different word and the values indicate the number of times that word appeared in the document. The overall size of a count vector matrix can be quite large (the number of columns is the total number of different words used across all documents in a corpus), but most entries in the matrix are zero (each document contains only a few of all the possible words). Therefore, it is most efficient to represent the count vectors as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10892\n",
      "(40000, 10892)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text and compute counts\n",
    "vocab_name = CountVectorizer(stop_words='english').fit(train_corpus_name)\n",
    "# generate counts for a new set of documents\n",
    "x_train_name = vocab_name.transform(train_corpus_name)\n",
    "x_test_name = vocab_name.transform(test_name)\n",
    "\n",
    "# check the number of words in vocabulary\n",
    "print(len(vocab_name.vocabulary_))\n",
    "# check the shape of sparse matrix\n",
    "print(x_train_name.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# load train and test set\n",
    "x_train_original = pd.read_csv(r\"recipe_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "x_test_original = pd.read_csv(r\"recipe_test.csv\", index_col = False, delimiter = ',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to remove punctuation\n",
    "a = r\"[\\\"\\',\\[\\]]\"\n",
    "x_train_original['name']=(x_train_original['name']).str.replace(a, \"\", regex=True)\n",
    "x_train_original['steps']=(x_train_original['steps']).str.replace(a, \"\", regex=True)\n",
    "x_train_original['ingredients']=(x_train_original['ingredients']).str.replace(a, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split recipe_train.csv\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_original,x_train_original.iloc[:,-1], test_size=0.23, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Count Vectorise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorize name, step and ingr for both train and test set & clean sentence with stopwords\n",
    "\n",
    "# vectorizes name\n",
    "name=(X_train['name'])\n",
    "name_test=(X_test['name'])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(name)\n",
    "names=vectorizer.transform(name)\n",
    "names_test=vectorizer.transform(name_test)\n",
    "\n",
    "#vectorizes step\n",
    "step_test=(X_test['steps'])\n",
    "step=(X_train['steps'])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(step)\n",
    "step=vectorizer.transform(step)\n",
    "step_test=vectorizer.transform(step_test)\n",
    "\n",
    "#vectorizes ingredients\n",
    "ingr_test=(X_test['ingredients'])\n",
    "ingr=(X_train['ingredients'])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(ingr)\n",
    "ingr=vectorizer.transform(ingr)\n",
    "ingr_test=vectorizer.transform(ingr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Chi Square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection using chi-square, selects the top 1000 features\n",
    "\n",
    "x2 = SelectKBest(chi2, k=1000)\n",
    "X_train_x2_names = x2.fit_transform(names,y_train)\n",
    "X_test_x2_names = x2.transform(names_test)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=1000)\n",
    "X_train_x2_step = x2.fit_transform(step,y_train)\n",
    "X_test_x2_step = x2.transform(step_test)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=1000)\n",
    "X_train_x2_ingr = x2.fit_transform(ingr,y_train)\n",
    "X_test_x2_ingr = x2.transform(ingr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_x2=hstack((X_train_x2_names, X_train_x2_step,X_train_x2_ingr,X_train[ 'n_steps'].to_numpy().reshape(-1,1),X_train[ 'n_ingredients'].to_numpy().reshape(-1,1)))\n",
    "all_features_test_x2=hstack((X_test_x2_names, X_test_x2_step,X_test_x2_ingr,X_test[ 'n_steps'].to_numpy().reshape(-1,1),X_test[ 'n_ingredients'].to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy for Chisquare: 0.7919565217391304\n"
     ]
    }
   ],
   "source": [
    "# train & test data using logistic regression model\n",
    "# lr = LogisticRegression(max_iter=2000).fit(all_features_x2, y_train)\n",
    "# print(\"Logistic regression accuracy for Chisquare:\",lr.score(all_features_test_x2,y_test))\n",
    "\n",
    "#tuning with C hyperparameter = 0.5\n",
    "lr_2 = LogisticRegression(max_iter=2000, C=0.5).fit(all_features_x2, y_train)\n",
    "print(\"Logistic regression accuracy for Chisquare:\",lr_2.score(all_features_test_x2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial accuracy for Chisquare: 0.7344565217391305\n"
     ]
    }
   ],
   "source": [
    "# train & test data using multinomial NB model alpha = 10\n",
    "\n",
    "Multi = MultinomialNB(alpha=10).fit(all_features_x2.toarray(), y_train)\n",
    "print(\"Multinomial accuracy for Chisquare:\", Multi.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB accuracy for Chisquare: 0.6475\n"
     ]
    }
   ],
   "source": [
    "# train & test data using gaussian NB model\n",
    "\n",
    "gaus= GaussianNB().fit(all_features_x2.toarray(), y_train)\n",
    "print(\"Gaussian NB accuracy for Chisquare:\",gaus.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli accuracy: 0.7515217391304347\n"
     ]
    }
   ],
   "source": [
    "# train & test data using bernoulli NB model with alpha = 6\n",
    "\n",
    "bernoulli= BernoulliNB(alpha=6).fit(all_features_x2.toarray(), y_train)\n",
    "print(\"bernoulli accuracy:\",bernoulli.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree accuracy: 0.7544565217391305\n"
     ]
    }
   ],
   "source": [
    "# train & test data using Decision Tree with max depth = 20\n",
    "\n",
    "dt= DecisionTreeClassifier(max_depth=20).fit(all_features_x2.toarray(), y_train)\n",
    "print(\"decision tree accuracy:\", dt.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train & test data using Neural Network\n",
    "\n",
    "clf = MLPClassifier(max_iter=2000)\n",
    "clf.fit(all_features_x2.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with relu activation function and alpha = 0.0001 and 100 neurons in 1st hidden layer: 0.7739130434782608\n"
     ]
    }
   ],
   "source": [
    "# without standardscaler\n",
    "\n",
    "print(\"NN with relu activation function and alpha = 0.0001 and 100 neurons in 1st hidden layer:\", clf.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(all_features_x2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train & test data using Neural Network with standardscaler\n",
    "clf = MLPClassifier(max_iter=2000, alpha=0.1)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with relu activation function and alpha = 0.0001 and 100 neurons in 1st hidden layer: 0.7682608695652174\n"
     ]
    }
   ],
   "source": [
    "x_test = scaler.fit_transform(all_features_test_x2.toarray())\n",
    "print(\"NN with relu activation function and alpha = 0.0001 and 100 neurons in 1st hidden layer:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.77      0.76      4046\n",
      "         2.0       0.79      0.78      0.78      4695\n",
      "         3.0       0.73      0.63      0.68       459\n",
      "\n",
      "    accuracy                           0.77      9200\n",
      "   macro avg       0.76      0.73      0.74      9200\n",
      "weighted avg       0.77      0.77      0.77      9200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,predictions)) # precision, recall, f1 score for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking method accuracy: 0.8027173913043478\n"
     ]
    }
   ],
   "source": [
    "# train & test data using stacking method with base Logistic Regression, Decision Tree & Multinomial NB as Base Model\n",
    "# and with Logistic Regression as learner model\n",
    "\n",
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter=5000, C=0.5)))\n",
    "level0.append(('dt', DecisionTreeClassifier(max_depth=10)))\n",
    "level0.append(('Multinomial NB', MultinomialNB(alpha=10)))\n",
    "\n",
    "# define meta learner model\n",
    "level1 = LogisticRegression(C=0.5,max_iter=5000)\n",
    "\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "# fit the model on all available data\n",
    "model.fit(all_features_x2.toarray(), y_train)\n",
    "\n",
    "print(\"stacking method accuracy:\",model.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking method accuracy: 0.8004347826086956\n"
     ]
    }
   ],
   "source": [
    "# train & test data using stacking method with base Logistic Regression, Decision Tree & Bernoulli NB as Base Model\n",
    "# and with Logistic Regression as learner model\n",
    "\n",
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter=5000, C=0.5)))\n",
    "level0.append(('dt', DecisionTreeClassifier(max_depth=10)))\n",
    "level0.append(('BNB', BernoulliNB(alpha=6)))\n",
    "\n",
    "# define meta learner model\n",
    "level1 = LogisticRegression(C=0.5,max_iter=5000)\n",
    "\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "# fit the model on all available data\n",
    "model.fit(all_features_x2.toarray(), y_train)\n",
    "\n",
    "print(\"stacking method accuracy:\",model.score(all_features_test_x2.toarray(),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Validation for Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorise\n",
    "Now, we use all the data set from recipe_train.csv for training instead of splitting them for kaggle test. Here, we are using the best model which is Ensemble Stacking to predict the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=x_train_original['duration_label']\n",
    "\n",
    "name=(x_train_original['name'])\n",
    "name_kaggle_test=(x_test_original['name'])\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(name)\n",
    "names=vectorizer.transform(name)\n",
    "name_kaggle_test=vectorizer.transform(name_kaggle_test)\n",
    "\n",
    "step_kaggle_test=(x_test_original['steps'])\n",
    "step=(x_train_original['steps'])\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(step)\n",
    "step=vectorizer.transform(step)\n",
    "step_kaggle_test=vectorizer.transform(step_kaggle_test)\n",
    "\n",
    "ingr_kaggle_test=(x_test_original['ingredients'])\n",
    "ingr=(x_train_original['ingredients'])\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(ingr)\n",
    "ingr=vectorizer.transform(ingr)\n",
    "ingr_kaggle_test=vectorizer.transform(ingr_kaggle_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Chisquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = SelectKBest(chi2, k=1000)\n",
    "names_x2 = x2.fit_transform(names,label)\n",
    "names_test_x2 = x2.transform(name_kaggle_test)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=1000)\n",
    "step_x2 = x2.fit_transform(step,label)\n",
    "step_test_x2 = x2.transform(step_kaggle_test)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=1000)\n",
    "ingr_x2 = x2.fit_transform(ingr,label)\n",
    "ingr_test_x2 = x2.transform(ingr_kaggle_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_train=hstack((names_x2, step_x2,ingr_x2,x_train_original[ 'n_steps'].to_numpy().reshape(-1,1),x_train_original[ 'n_ingredients'].to_numpy().reshape(-1,1)))\n",
    "all_features_test=hstack((names_test_x2, step_test_x2,ingr_test_x2,x_test_original[ 'n_steps'].to_numpy().reshape(-1,1),x_test_original[ 'n_ingredients'].to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=2000).fit(all_features_train, label)\n",
    "pred_test=lr.predict(all_features_test)\n",
    "\n",
    "d2 = { 'duration_label':pred_test}\n",
    "df2=pd.DataFrame(d2)\n",
    "df2.index+=1\n",
    "index=df2.index\n",
    "index.name = \"id\"\n",
    "df2.to_csv ('logistic_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune C hyperparameter\n",
    "lr_2 = LogisticRegression(max_iter=2000, C=0.5).fit(all_features_train, label)\n",
    "pred_test=lr_2.predict(all_features_test)\n",
    "\n",
    "d2 = { 'duration_label':pred_test}\n",
    "df2=pd.DataFrame(d2)\n",
    "df2.index+=1\n",
    "index=df2.index\n",
    "index.name = \"id\"\n",
    "df2.to_csv ('logistic_tune_parameter_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB(alpha=10).fit(all_features_train, label)\n",
    "pred_test=MNB.predict(all_features_test)\n",
    "\n",
    "d3 = { 'duration_label':pred_test}\n",
    "df3=pd.DataFrame(d3)\n",
    "df3.index+=1\n",
    "index=df3.index\n",
    "index.name = \"id\"\n",
    "df3.to_csv ('MultinomialNB_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNB = BernoulliNB(alpha=6).fit(all_features_train.toarray(), label)\n",
    "pred_test=BNB.predict(all_features_test.toarray())\n",
    "\n",
    "d4 = { 'duration_label':pred_test}\n",
    "df4=pd.DataFrame(d4)\n",
    "df4.index+=1\n",
    "index=df4.index\n",
    "index.name = \"id\"\n",
    "df4.to_csv('BernoulliNB_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB = GaussianNB().fit(all_features_train.toarray(), label)\n",
    "pred_test=GNB.predict(all_features_test.toarray())\n",
    "\n",
    "d5 = { 'duration_label':pred_test}\n",
    "df5=pd.DataFrame(d5)\n",
    "df5.index+=1\n",
    "index=df5.index\n",
    "index.name = \"id\"\n",
    "df5.to_csv ('GaussianNB_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=20).fit(all_features_train, label)\n",
    "pred_test=dt.predict(all_features_test)\n",
    "\n",
    "d6 = { 'duration_label':pred_test}\n",
    "df6=pd.DataFrame(d6)\n",
    "df6.index+=1\n",
    "index=df6.index\n",
    "index.name = \"id\"\n",
    "df6.to_csv ('Decison_tree_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(max_iter=2000)\n",
    "clf.fit(all_features_train.toarray(), label)\n",
    "pred_test=clf.predict(all_features_test)\n",
    "d7 = { 'duration_label':pred_test}\n",
    "df7=pd.DataFrame(d7)\n",
    "df7.index+=1\n",
    "index=df7.index\n",
    "index.name = \"id\"\n",
    "df7.to_csv ('Neural_Network_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr',\n",
       "                                LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                   dual=False,\n",
       "                                                   fit_intercept=True,\n",
       "                                                   intercept_scaling=1,\n",
       "                                                   l1_ratio=None, max_iter=2000,\n",
       "                                                   multi_class='auto',\n",
       "                                                   n_jobs=None, penalty='l2',\n",
       "                                                   random_state=None,\n",
       "                                                   solver='lbfgs', tol=0.0001,\n",
       "                                                   verbose=0,\n",
       "                                                   warm_start=False)),\n",
       "                               ('dt',\n",
       "                                DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                       class_weight=None,\n",
       "                                                       criterion...\n",
       "                                MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                              fit_prior=True))],\n",
       "                   final_estimator=LogisticRegression(C=1, class_weight=None,\n",
       "                                                      dual=False,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      intercept_scaling=1,\n",
       "                                                      l1_ratio=None,\n",
       "                                                      max_iter=2000,\n",
       "                                                      multi_class='auto',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='lbfgs',\n",
       "                                                      tol=0.0001, verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking using Linear Regression, Decision Tree and ... as the basemodel and Logistic Regression as the learner model\n",
    "\n",
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter=2000)))\n",
    "level0.append(('dt', DecisionTreeClassifier(max_depth=20)))\n",
    "level0.append(('MNB', MultinomialNB()))\n",
    "#level0.append(('svm', svm.LinearSVC(max_iter=5000)))\n",
    "#level0.append(('svc', svm.LinearSVC(max_iter=5000)))\n",
    "#level0.append(('MLPClassifier', MLPClassifier(max_iter=2000)))\n",
    "\n",
    "# define meta learner model\n",
    "level1 = LogisticRegression(C=1,max_iter=2000)\n",
    "\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "# fit the model on all available data\n",
    "\n",
    "# fit the model on all available data\n",
    "model.fit(all_features_train.toarray(), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test set (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1. ... 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=model.predict(all_features_test.toarray())\n",
    "\n",
    "d1 = { 'duration_label':y_pred_test}\n",
    "df1=pd.DataFrame(d1)\n",
    "df1.index+=1\n",
    "index=df1.index\n",
    "index.name = \"id\"\n",
    "df1.to_csv ('kaggle_dataframe.csv') # export prediction results as dataframe\n",
    "\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "doc2vec methods are an extension of word2vec. word2vec maps words to a high-dimensional vector space in such a way that words which appear in similar contexts will be close together in the space. doc2vec does a similar embedding for multi-word passages. The doc2vec (or Paragraph Vector) method was introduced by:\n",
    "\n",
    "**Le & Mikolov (2014)** Distributed Representations of Sentences and Documents<br>\n",
    "https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "The implementation of doc2vec used for this project is from gensim and documented here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "The size of the output vector is a free parameter. Most implemementations use around 100-300 dimensions, but the best size depends on the problem you're trying to solve with the embeddings and the number of training samples, so you may wish to try different vector sizes. We provided three sets of doc2vec features, and their dimensions are 50 and 100. The vectors themselves represent directions in a high-dimensional concept space; the columns do not represent specific words or phrases. Values in the vector are continuous real numbers and can be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# size of the output vector\n",
    "vec_size = 20\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False):\n",
    "    for i, line in enumerate(txt):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "# tokenize a training corpus\n",
    "corpus_name = list(tokenize_corpus(train_corpus_name))\n",
    "\n",
    "# train doc2vec on the training corpus\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus_name)\n",
    "model.train(corpus_name, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# tokenize new documents\n",
    "doc = list(tokenize_corpus(test_name, tokens_only=True))\n",
    "\n",
    "# generate embeddings for the new documents\n",
    "x_test_name = np.zeros((len(doc),vec_size))\n",
    "for i in range(len(doc)):\n",
    "    x_test_name[i,:] = model.infer_vector(doc[i])\n",
    "    \n",
    "# check the shape of doc_emb\n",
    "print(x_test_name.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
